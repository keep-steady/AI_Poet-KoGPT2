{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Transfer_Learning!.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPTeA8MnzM6EBmNeEdVWF68",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newfull5/AI_Poet-KoGPT2/blob/master/2.%20Transfer_Learning!.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NqzTZi9MA3e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "d701ef99-d538-4c09-86bb-79d407c3da9a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hih5UDHjFe5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "0dbbb246-48e6-4e2e-af75-f7356a97a9b5"
      },
      "source": [
        "!git clone https://github.com/SKT-AI/KoGPT2.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'KoGPT2'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 61 (delta 30), reused 40 (delta 18), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (61/61), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v258y-mtLKBj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16f2e33f-c125-4aad-fea0-41f55d53d1ff"
      },
      "source": [
        "!cd KoGPT2 && pip install -r requirements.txt\n",
        "!cd KoGPT2 && pip install ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gluonnlp>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/27/07b57d22496ed6c98b247e578712122402487f5c265ec70a747900f97060/gluonnlp-0.9.1.tar.gz (252kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 14.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256kB 3.4MB/s \n",
            "\u001b[?25hCollecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f5/d79b5b40735086ff1100c680703e0f3efc830fa455e268e9e96f3c857e93/mxnet-1.6.0-py2.py3-none-any.whl (68.7MB)\n",
            "\u001b[K     |████████████████████████████████| 68.7MB 41kB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.5.0+cu101)\n",
            "Collecting transformers==2.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 28.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r requirements.txt (line 1)) (1.18.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r requirements.txt (line 1)) (0.29.17)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r requirements.txt (line 1)) (20.3)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->-r requirements.txt (line 5)) (2019.12.20)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->-r requirements.txt (line 5)) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 28.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->-r requirements.txt (line 5)) (1.13.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->-r requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r requirements.txt (line 2)) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r requirements.txt (line 2)) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1->-r requirements.txt (line 5)) (0.15.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.1->-r requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.10 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.1->-r requirements.txt (line 5)) (1.16.10)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.1->-r requirements.txt (line 5)) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.10->boto3->transformers==2.4.1->-r requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.10->boto3->transformers==2.4.1->-r requirements.txt (line 5)) (0.15.2)\n",
            "Building wheels for collected packages: gluonnlp, sacremoses\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp36-cp36m-linux_x86_64.whl size=471063 sha256=0f1b414aa462a9e9b6e5536116f781d78a8680aea492da35a9df44962e047f8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/60/16/1f8a40e68b85bd9bd7960e91830bca5e40cd113f3220b7e231\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=54a6f5f3373d9c71bdc53281adaf2ee5ceb2ecef4c31e3c578609b299be04e98\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built gluonnlp sacremoses\n",
            "Installing collected packages: gluonnlp, graphviz, mxnet, sentencepiece, tokenizers, sacremoses, transformers\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed gluonnlp-0.9.1 graphviz-0.8.4 mxnet-1.6.0 sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.0.11 transformers-2.4.1\n",
            "Processing /content/KoGPT2\n",
            "Building wheels for collected packages: kogpt2\n",
            "  Building wheel for kogpt2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kogpt2: filename=kogpt2-0.1.0-cp36-none-any.whl size=22256 sha256=06872e73d5cfbbd400e78a64e82dd3cebaf6e63d334414a8c3c37a109624fd9f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xmltlwmj/wheels/ac/e5/83/e839e6a987261c05b2e32cbd9770007e19f8ea7e2f2f7b9d3c\n",
            "Successfully built kogpt2\n",
            "Installing collected packages: kogpt2\n",
            "Successfully installed kogpt2-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgFR_9HaLNLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from kogpt2.pytorch_kogpt2 import get_pytorch_kogpt2_model\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "from kogpt2.utils import get_tokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gklkGDbcQrsV",
        "colab_type": "code",
        "outputId": "7d277bf2-34a5-42c7-c4ba-562fa4eeffc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if torch.cuda.device_count():\n",
        "  print('구글 만세!')\n",
        "  PU = 'cuda'\n",
        "else:\n",
        "  PU = 'cpu'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "구글 만세!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VprW-gq4BMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#토큰화와 인덱싱을해서 리턴하는 함수\n",
        "\n",
        "def dataset (file_path):\n",
        "  data = []\n",
        "  tokenizer = SentencepieceTokenizer(get_tokenizer())\n",
        "  f = open(file_path,'r',encoding='utf-8')\n",
        "\n",
        "  while True:\n",
        "    file = f.readline()\n",
        "\n",
        "    if not file:\n",
        "      break\n",
        "    line = tokenizer(file)\n",
        "    indexing_word = [vocab[vocab.bos_token]]+ vocab[line] + [vocab[vocab.eos_token]]\n",
        "    data.append(indexing_word)\n",
        "\n",
        "  f.close()\n",
        "\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvdfVej0MyXd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1266a07c-7e0b-414f-a559-97c1461efb6a"
      },
      "source": [
        "_, vocab = get_pytorch_kogpt2_model()\n",
        "\n",
        "#model.to(torch.device(PU)) #모델 연산 유닛 설정\n",
        "#model.train() #모델 학습모드로 변경\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BblXdDzxwU1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import subprocess\n",
        "\n",
        "def get_gpu_memory_map():\n",
        "\t\"\"\"Get the current gpu usage.\n",
        "\tReturns\n",
        "\t-------\n",
        "\tusage: dict\n",
        "\t\tKeys are device ids as integers.\n",
        "\t\tValues are memory usage as integers in MB.\n",
        "\t\"\"\"\n",
        "\tresult = subprocess.check_output(\n",
        "\t\t[\n",
        "\t\t\t'nvidia-smi', '--query-gpu=memory.used',\n",
        "\t\t\t'--format=csv,nounits,noheader'\n",
        "\t\t], encoding='utf-8')\n",
        "\t# Convert lines into a dictionary\n",
        "\tgpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
        "\tgpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
        "\treturn gpu_memory_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gwircF7wj8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "17b215c9-bd18-4566-afdc-a339f5f9006d"
      },
      "source": [
        "get_gpu_memory_map()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 11422}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzZ6pohGYtqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#모델 불러오기 코드 입니다\n",
        "from kogpt2.model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "save_path = 'drive/My Drive/Colab Notebooks/KoGPT2_checkpoint/'\n",
        "\n",
        "kogpt2_config = {\n",
        "\t\t\"initializer_range\": 0.02,\n",
        "\t\t\"layer_norm_epsilon\": 0.000025,\n",
        "\t\t\"n_ctx\": 1024,\n",
        "\t\t\"n_embd\": 768,\n",
        "\t\t\"n_head\": 12,\n",
        "\t\t\"n_layer\": 12,\n",
        "\t\t\"n_positions\": 1024,\n",
        "\t\t\"vocab_size\": 50000\n",
        "}\n",
        "\n",
        "checkpoint = torch.load(save_path+'KoGPT2_checkpoint.tar', map_location=PU)\n",
        "\n",
        "kogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
        "\n",
        "kogpt2model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "kogpt2model.train()\n",
        "\n",
        "kogpt2model.to(torch.device(PU))\n",
        "\n",
        "model = kogpt2model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwBjyXTToPkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#torch.save(model.state_dict,'model_state_dict.pth') #모델의 가중치 값을 저장하는 코드입니다.\n",
        "#model.load_state_dict(torch.load(save_path+'KoGPT2_checkpoint.tar')) #모델의 가중치 값을 불러오는 코드입니다.\n",
        "\n",
        "#torch.save(model, PATH) #모델 전체를 저장하는 코드입니다.\n",
        "#model = torch.load(PATH) #모델 전체를 불러오는 코드입니다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWQu0-3q36El",
        "colab_type": "code",
        "outputId": "1efdca3a-566c-4706-d822-882c087fd0ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "file_path = 'drive/My Drive/poem_data/dataset_combined.txt'\n",
        "dataset = DataLoader(dataset(file_path), batch_size=100, shuffle=True, pin_memory=True)\n",
        "\n",
        "learning_rate = 0.000025\n",
        "epochs = 1000\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRJYgxEZ57ZE",
        "colab_type": "code",
        "outputId": "acc1fd97-94c4-4c3c-a08d-d766ca930364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "for epoch in range(checkpoint['epoch'], epochs+1):\n",
        "  cnt = 0\n",
        "\n",
        "  for data in dataset:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    data = torch.stack(data)\n",
        "    data = data.transpose(1,0)\n",
        "    data = data.to(PU)\n",
        "\n",
        "    output = model(data,labels=data)\n",
        "    loss, logits = output[:2]\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if cnt % 20 == 0:\n",
        "      print(\"[+] epoch : {}, cnt : {}, loss : {} [+]\".format(epoch, cnt+1, str(loss)[7:12]))\n",
        "\n",
        "    if epoch % 20 == 0 and cnt == 1:\n",
        "      torch.save({\n",
        "          'epoch': epoch,\n",
        "          'cnt': cnt,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'loss': loss,\n",
        "          }, save_path+'KoGPT2_checkpoint.tar')\n",
        "      \n",
        "    cnt += 1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[+] epoch : 744, cnt : 261, loss : 4.914 [+]\n",
            "[+] epoch : 745, cnt : 1, loss : 4.770 [+]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ded7f4d8bacf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLhK8TMCApPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}