{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Generater.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/8eJkR1GzM3NEVOtvODgl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newfull5/AI_Poet-KoGPT2/blob/master/3_Generater.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lbu_X17pLQK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6231f74-fa09-4a0d-9e61-67df2da70278"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idc5kfNzpPIG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95e731dd-abfc-427b-f459-19a59ac4226c"
      },
      "source": [
        "!git clone https://github.com/SKT-AI/KoGPT2.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'KoGPT2' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKkHQPlmp2SF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd KoGPT2 && pip install -r requirements.txt\n",
        "!cd KoGPT2 && pip install ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4vrUGkBp7DZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from kogpt2.pytorch_kogpt2 import get_pytorch_kogpt2_model\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "from kogpt2.utils import get_tokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJWI3XmoqQW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.device_count():\n",
        "  PU = 'cuda'\n",
        "else:\n",
        "  PU = 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWqO4fu_qWyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PU = 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLlWKPpxqXoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#토큰화와 인덱싱을해서 리턴하는 함수\n",
        "\n",
        "def dataset (file_path):\n",
        "  data = []\n",
        "  tokenizer = SentencepieceTokenizer(get_tokenizer())\n",
        "  f = open(file_path,'r',encoding='utf-8')\n",
        "\n",
        "  while True:\n",
        "    file = f.readline()\n",
        "\n",
        "    if not file:\n",
        "      break\n",
        "    line = tokenizer(file)\n",
        "    indexing_word = [vocab[vocab.bos_token]]+ vocab[line] + [vocab[vocab.eos_token]]\n",
        "    data.append(indexing_word)\n",
        "\n",
        "  f.close()\n",
        "\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKI_hmnnqjPo",
        "colab_type": "code",
        "outputId": "7d1d6d98-9046-445f-861b-a4b27b089dd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model, vocab = get_pytorch_kogpt2_model()\n",
        "\n",
        "load_path = 'drive/My Drive/Colab Notebooks/KoGPT2_checkpoint/KoGPT2_checkpoint.tar'\n",
        "checkpoint = torch.load(load_path, map_location=torch.device(PU))\n",
        "\n",
        "model.to(torch.device(PU)) #모델 연산 유닛 설정\n",
        "torch.load(load_path, map_location=torch.device(PU))\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "using cached model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50000, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbvPvKKVql1G",
        "colab_type": "code",
        "outputId": "35b0c136-bd4d-4b5d-ca58-d922a191c6ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Tokenizer = SentencepieceTokenizer(get_tokenizer())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6Kapq95tH59",
        "colab_type": "code",
        "outputId": "0dc22098-9465-4b5d-8f13-f8f792fd73a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        }
      },
      "source": [
        "sent =''\n",
        "while 1:\n",
        "\n",
        "  tmp_sent = input(' ')\n",
        "  sent = sent+tmp_sent\n",
        "\n",
        "  toked = Tokenizer(sent)\n",
        "  count = 0\n",
        "  generated_text =''\n",
        "  input_size = 50\n",
        "\n",
        "  if len(toked) >1022:\n",
        "    break\n",
        "\n",
        "  while 1:\n",
        "    input_ids = torch.tensor([vocab[vocab.bos_token],]  + vocab[toked]).unsqueeze(0)\n",
        "    predicts = model(input_ids)\n",
        "    pred = predicts[0]\n",
        "    # print('predicts:', torch.argmax(pred, axis=-1).squeeze())\n",
        "    gen = vocab.to_tokens(torch.argmax(pred, axis=-1).squeeze().tolist())[-1]\n",
        "    if gen == '</s>':\n",
        "      print('to_tokens:',vocab.to_tokens(torch.argmax(pred, axis=-1).squeeze().tolist()))\n",
        "    if gen == '.' or count>input_size:\n",
        "      sent += gen.replace('▁', ' ')\n",
        "      generated_text += gen.replace('▁', ' ')\n",
        "      sent += '\\n'\n",
        "      generated_text += '\\n'\n",
        "      toked = Tokenizer(sent)\n",
        "      count =0\n",
        "      break\n",
        "      # print('to_tokens:',vocab.to_tokens(torch.argmax(pred, axis=-1).squeeze().tolist()))\n",
        "    # if count >= input_size:\n",
        "    #   break\n",
        "    sent += gen.replace('▁', ' ')\n",
        "    generated_text += gen.replace('▁', ' ')\n",
        "    # print(generated_text)\n",
        "\n",
        "    toked = Tokenizer(sent)\n",
        "    count += 1\n",
        "  print(generated_text)\n",
        "  generated_text=''\n",
        "print(sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 배고프다\n",
            ".\n",
            "\n",
            " 왜 대답이 없니\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '</s>']\n",
            "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>                                \n",
            "\n",
            " 미치겠군\n",
            ".\n",
            "\n",
            " .\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '▁', '도록', '어', '.', '</s>', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '▁', '도록', '어', '.', '</s>', '</s>', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '▁', '도록', '어', '.', '</s>', '</s>', 's', '></', 'sm', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '▁', '도록', '어', '.', '</s>', '</s>', 's', '></', 'sm', '></', 'sm', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '▁', '도록', '어', '.', '</s>', '</s>', 's', '></', 'sm', '></', 'sm', '></', 'sm', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '▁', '도록', '어', '.', '</s>', '</s>', 's', '></', 'sm', '></', 'sm', '></', 'sm', '></', 'sm', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '▁', '도록', '어', '.', '</s>', '</s>', 's', '></', 'sm', '></', 'sm', '></', 'sm', '></', 'sm', '></', 'sm', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '▁', '도록', '어', '.', '</s>', '</s>', 's', '></', 'sm', '></', 'sm', '></', 'sm', '></', 'sm', '></', 'sm', '></', 's', '></', '</s>']\n",
            "to_tokens: ['▁나는', '픈', '고', '.', '</s>', '▁가', '이', '▁없나', '느냐', '</s>', 'br', 'om', 'f', '></', 'f', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', 's', '></', '▁', '도록', '어', '.', '</s>', '</s>', 's', '></', 'sm', '></', 'sm', '></', 'sm', '></', 'sm', '></', 'sm', '></', 's', '></', 's', '></', '</s>']\n",
            "</s></s></s></s></s></s></s></s></s>                                           \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul_dzgpvtgpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}